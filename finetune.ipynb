{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e550fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import peft\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,)\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de221a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/data params\n",
    "base_model: str = \"/ibex/user/radhaks/LLMs/LLaMA_7B/Alpaca\"  # the only required argument\n",
    "data_path: str = \"/ibex/user/radhaks/LLMs/LLaMA_7B/alpaca-lora-srijith/data/train_data_all_three.json\" # alpaca_data_cleaned_archive.json\"\n",
    "wandb_run_name: str = \"Debugging_session\"\n",
    "\n",
    "output_dir: str = os.path.join(\"/ibex/user/radhaks/LLMs/LLaMA_7B/alpaca-lora-srijith/weights\", wandb_run_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "# training hyperparams\n",
    "batch_size: int = 128\n",
    "micro_batch_size: int = 4 # batch size per GPU \n",
    "num_epochs: int = 5\n",
    "learning_rate: float = 3e-4\n",
    "cutoff_len: int = 2000 # will have to change this\n",
    "val_set_size: int = 20 # amount of val datapoints to steal from your train datapoints\n",
    "    \n",
    "# lora hyperparams - did not change\n",
    "lora_r: int = 8\n",
    "lora_alpha: int = 16\n",
    "lora_dropout: float = 0.05\n",
    "lora_target_modules: List[str] = [\"q_proj\",\"v_proj\",]\n",
    "    \n",
    "# llm hyperparams LOT of mistakes here as these are totally ignored in the funtions below - check out alpaca code\n",
    "train_on_inputs: bool = True  # if False, masks out inputs in loss\n",
    "add_eos_token: bool = True # was false COULD CAUSE POTENTIAL PROBLEMS\n",
    "group_by_length: bool = False  # faster, but produces an odd training loss curve\n",
    "# wandb params\n",
    "wandb_project: str = \"Alpaca_Lora_fine-tuning\"\n",
    "\n",
    "wandb_watch: str = \"false\"  # options: false | gradients | all\n",
    "wandb_log_model: str = \"true\"  # options: false | true\n",
    "resume_from_checkpoint: str = None  # either training checkpoint or final adapter\n",
    "prompt_template_name: str = \"alpaca\"  # The prompt template to use, will default to alpaca.\n",
    "\n",
    "if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"val_set_size: {val_set_size}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        # THINGS TO DO 1\n",
    "        f\"wandb_project: {wandb_project}\\n\"\n",
    "        f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "        f\"wandb_watch: {wandb_watch}\\n\"\n",
    "        f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "assert (\n",
    "    base_model\n",
    "), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "prompter = Prompter(prompt_template_name)\n",
    "\n",
    "device_map = \"auto\"\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "ddp = world_size != 1\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "# THINGS TO DO 1\n",
    "# Check if parameter passed or if set within environ\n",
    "use_wandb = len(wandb_project) > 0 or (\n",
    "    \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    ")\n",
    "# THINGS TO DO 1\n",
    "# Only overwrite environ if wandb param passed\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "if len(wandb_watch) > 0:\n",
    "    os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "if len(wandb_log_model) > 0:\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14168faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "tokenizer.pad_token_id = (0)  # unk. we want this to be different from the eos token -check alpaca code first\n",
    "tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "\n",
    "# tokenize the prompt witout padding and add eos_token. Also add a dictionary field \n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < cutoff_len and add_eos_token): # \n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c036f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \n",
    "    # full_prompt = prompter.generate_prompt(\n",
    "    #     data_point[\"instruction\"],\n",
    "    #     data_point[\"input\"],\n",
    "    #     data_point[\"output\"],\n",
    "    # )\n",
    "    \n",
    "    full_prompt = data_point['prompt']\n",
    "    \n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if False:#not train_on_inputs:  # all your promts have inputs\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving all the data stuff here\n",
    "data_path = '/ibex/user/radhaks/LLMs/LLaMA_7B/alpaca-lora-srijith/data/train_data_all_three.json'\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "    \n",
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=True, seed=42\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_int8_training__(model, use_gradient_checkpointing=True):\n",
    "    r\"\"\"\n",
    "    This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
    "        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n",
    "        head to fp32\n",
    "\n",
    "    Args:\n",
    "        model, (`transformers.PreTrainedModel`):\n",
    "            The loaded model from `transformers`\n",
    "    \"\"\"\n",
    "    loaded_in_8bit = getattr(model, \"is_loaded_in_8bit\", False)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # freeze base model's layers\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # cast all non INT8 parameters to fp32\n",
    "    for param in model.parameters():\n",
    "        if (param.dtype == torch.float16) or (param.dtype == torch.bfloat16):\n",
    "            param.data = param.data.to(torch.float16)\n",
    "\n",
    "    if loaded_in_8bit and use_gradient_checkpointing:\n",
    "        # For backward compatibility\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "        else:\n",
    "\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "\n",
    "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "        # enable gradient checkpointing for memory efficiency\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76573de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training__(model) \n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     # freeze base model's layers\n",
    "#     param.requires_grad = False\n",
    "#     if getattr(model, \"is_loaded_in_8bit\", False):\n",
    "#         if param.ndim == 1 and \"layer_norm\" in name:\n",
    "#             param.data = param.data.to(torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "# model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    # Check the available weights and load them\n",
    "    checkpoint_name = os.path.join(\n",
    "        resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "    )  # Full checkpoint\n",
    "    if not os.path.exists(checkpoint_name):\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"adapter_model.bin\"\n",
    "        )  # only LoRA model - LoRA config above has to fit\n",
    "        resume_from_checkpoint = (\n",
    "            False  # So the trainer won't try loading its state\n",
    "        )\n",
    "    # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "    if os.path.exists(checkpoint_name):\n",
    "        print(f\"Restarting from {checkpoint_name}\")\n",
    "        adapters_weights = torch.load(checkpoint_name)\n",
    "        set_peft_model_state_dict(model, adapters_weights)\n",
    "    else:\n",
    "        print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "\n",
    "if not ddp and torch.cuda.device_count() > 1:\n",
    "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=True,\n",
    "        logging_steps=2,\n",
    "        eval_steps=2 if val_set_size > 0 else None,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        output_dir=output_dir,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "        group_by_length=group_by_length,\n",
    "        report_to=\"wandb\" if use_wandb else None,\n",
    "        run_name= wandb_run_name if use_wandb else None,),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\n If there's a warning about missing keys above, please disregard :)\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dae06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04ec27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
